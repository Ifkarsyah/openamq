primary partition
    - fully interconnected
    - each server is client of each other server
    - servers can be elected to root
    - servers share primary config data
    OK

secondary partition
    - fully connected to primary partition
    - each server is client of each primary server
    - servers cannot be elected to root
    - servers share primary config data
    OK

root server is most senior primary server
    - takes over when has unanimous vote of active primary servers
    - and active voters are >50% of known primary servers
    - secondary servers do not vote (may change)
    OK

voting procedure
    - each server does its own election
    - based on own SPID and SPIDs of all known peers
        - we expect same election results across all cluster
    OK
    
for best results
    - use 3 or more primary servers
    - all servers have identical cluster configuration
    OK

inbound cluster connection
    - clients logged in as cluster/cluster
    - provide callback and spid at connection time
        - amq_cluster then adds to peer list if not already known
    - may create multiple peer links back to same server
        - duplicates are removed when peer becomes active
        - no active duplicates in peer list
    OK

each cluster controller knows which peer is root
    - when server decides to become root it announces the fact
    - Cluster.Root method to all peers 
    - new peer gets root information via state replication
    OK

method wrapping
    - server method is wrapped as binary bucket
    - placed into cluster-class content
    - primary-> all
    - secondard-> root, (root becomes originator), root-> all
    OK
    
state replication
    - certain AMQP methods are replicated to all servers
        - from primary to all nodes
        - from secondary to all primaries
            - to all nodes except originator via root
    - all such methods form the "cluster state"
    - all primary servers hold list of cluster state methods
    - when new server connects to root
        - receives Cluster.Root message
        - receives full cluster state
        - excluding application messages
    OK

exchange management
    - exchanges are held globally
        - exchange.declare, exchange.delete replicated cluster-wide
        - stateful, replayed onto new servers
    - no guarantee of consistency if application is inconsistent
    OK
    
queue management
    - each server holds all global queues plus own private queues
    - shared queues (exclusive = 0) are held globally
        - queue.declare, queue.delete replicated cluster-wide
        - stateful, replayed onto new servers
    - no guarantee of consistency if application is inconsistent
        - i.e. queue declared differently on different servers
    - queue.purges are replicated iff queue is shared
        - not stateful
    - shared queues are owned by user 0
        - cannot be accessed through exclusive consumer
            -> consider removing exclusive consumer function
    OK

queue bindings
    - queue.binds are replicated if queue is shared
        - stateful, replayed onto new servers
        - shared queues are bound to all exchanges on all nodes
        OK
    - queue.binds are meta-replicated if queue is private
        - cluster.bind 
        - exchange name is same on both machines
        - stateful, replayed onto new servers
    OK

publish message
    - set Cluster-Id property to incoming message
        - [R/B/S]/spid/connection-id/channel_nbr
        - for returning messages to original producer
        - for pushing messages through cluster
    - publish to exchange/binding as normal
    - binding will publish to queues and to remote peers
    binding to queue:
        - if queue shared & cluster enabled & we are not root
            - if message came from application
                - push message to root node
        - else
            - accept message
    binding to peer:
        - if message came from application
            - push message to all bound peers
            - PUSH_ALL
        - if message came from secondary node & we are root
            - push message to all bound secondary peers
            - except where peer->spid == connection->client_spid
            - PUSH_SECONDARY
    OK

cluster key
    - string specified in server config and by application
    - specified by application at connection open time
        - Connection.Open.cluster_key (string)
    - verified by cluster connection manager
    - must also match for intra-cluster connections
    OK

class.consume, deliver
    0. create consumer as usual
        OK
    1. client provides user tag in all consume methods
        OK
    2. user tag returned with consume-ok and deliver methods
        OK
    3. if shared queue, broadcast cluster.consume
        - insert outbound consumer-tag into secondary consume
        OK

    4. when deliver method arrives
        -> client key-> consumer
            - connection->identifier,
            - channel->key,
            - consumer->tag
    - no-local?

    - need to cancel remote consumer if local consumer goes away


class.get, get-ok
    

    - when content arrives on proxy connection
        - garbage collect consumer if zombied
        - otherwise send to consumer as today


                    
        

Clarify in doc:
    - client can send synchronous methods without waiting for responses
    - responses MAY come out of order...
    - do we add a response token to all synchronous methods?
        - carried in replies?
    - ...? could be useful for knowing what consume-ok matches what consume...
    - ...? client may get consume ok's in wrong order today
    - ...? server could wait to process new input until output is sent
    


    - if returned,
        - if origin-spid is known
            - return to peer
        - else
            - return to root
    - on delivery to consumer
        - remove origin-spid header
        

    
    ... pass messages down across remote binds
    ... register remote consumers
    ... pass messages across consumers
    ... fix clients to connect to one of random server

    ... how to return messages if no immediate consumers?
        - need to add original connection & channel number to message
        - need to process incoming returns

client connection
    - if cluster enabled but not ready, wait
        - not ready = no root server defined yet
        - hold onto client connection...
    - if cluster disabled
        - pause client activity
        - client application can timeout if bored
    OK

    - add force option to C API
    - cluster needs to balance clients over all servers...
        - reconnect to (random) server
    - if cluster goes down, all connections should be closed
    - need to add heartbeating to proxy link to double-check
        --- sometimes when root goes down, we don't see it...


console issues
    - replicate configuration across cluster
    - enable/disable root on specific server


Figures

We can measure 'Internal Switching Rate' (ISR) from a faucet into sinks
(queues that have no consumers, so discard their messages).

With a single broker we can switch into Q queues, giving a ratio of Q,
and a tentative capacity of Q.10KM/s. When Q is 16 this gives us a
tentative ISR of 160KM/s, or 4.61TM/wd (per working day of 8 hours).

Per broker, the value of Q can reasonably go up to 256 or higher. The
XSR will depend on the network capacity of a broker but will always be
limited to the ISR. AMQ is designed for values of Q as high as 10,000.

With multiple brokers B fanning out from a root broker, we calculate the
ISR ratio as B.Q. With B = 6, and Q = 16, we have a tentative ISR of
960KM/s, or 27.65TM/wd.

With larger values for Q, we get extremely high ISRs and the cluster
efficiency approaches 100%. However these ISR measurements do not
translate into effective XSRs due to impossibiity of shifting that
amount of data across broker network links.

Maximum practical value for Q is 12, after which the XSR cannot keep up
with ISR, and this assumes 1.0Gb of bandwidth per broker.

Higher values of Q are feasible if message selectors are used to prune
messages arriving in sinks.

Assuming a maximum Q of 12, we get higher XSRs by increasing B and the
number of cluster peers. The cluster size will be 1 + B, and the limit
of B is identical to the limit of Q, namely 12. The maximum XSR in this
case (B = 12, Q = 12) is 1.44 MM/s and requires a cluster of 13 nodes.

Assuming a less efficient network between brokers and external clients,
with 30% of GB capacity, the maximum value for Q decreases to 4 and the
XSR correspondingly (480 KM/s). If the internal cluster network is also
less efficient, we arrive at B = Q = 4.

A reasonable configuration for testing cluster ISR is:

 - B = 7, Q = 12, cluster size = 8 plus one external faucet.

A reasonable configuration for testing cluster XSR is:

 - B = 3, Q = 4, cluster size = 4 plus one external faucet and
   12 external clients.

Note that the XSR/ISR estimates are based on a message size of 1000 bytes
and a broker capacity of 10 KM/s.  More realistic figures would be
message size of 250 bytes and capacity of 25 KM/s.


Test scenarios

 - test that exchanges are replicated
 - create exchange on one node, bind queue to exchange on other
    - root to root
    - root to backup
    - root to self
    - backup to root
    - backup to backup
    - backup to self
    - backup to other backup
    - secondary to root
    - secondary to backup
    - secondary to self
    - secondary to other secondary

 - same test on shared queue
    - create shared queue on one node, consume from queue on other

 - test clustered private queues
    - create private queue on each node
    - publish to each node
    - check messages arrive in each queue

 - test clustered shared queues
    - create shared queue
    - consume from shared queue once on each node
    - publish to shared queue
    - check messages distributed equally


