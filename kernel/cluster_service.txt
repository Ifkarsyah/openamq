changes
    - all servers specified in config file
        - servers share same config file
    - spid specified on command line
        - -c not specified -> not cluster
        - amq_cluster exists or does not
        
    - first server is primary
        - when starting, connect to all servers
        - we know if we're root...
        
- proxy list is totally static
    - when connected, verify spid, abort if wrong

    - calculate checksum of list
        - include in heartbeat
        - abort if not same as other server
    
- remove manual spid option
- new connection strategy
    - all fully interconnected, period.
    - use bridging for WAN deployment
    
- configuration:
    - server type: primary, backup, fanout
    - known_hosts list
        - at least one server...
        - comma-delimited 
        
- proxy management
    - outgoing proxies to all known hosts
    - incoming proxies from all known hosts
    - these two lists are built asynchronously
        - incoming cluster connection -> incoming proxy
    - we can review the lists to match them up
    - each outgoing proxy should match an incoming one

    - outgoing proxy properties:
        - host name (unresolved)
        - peer spid, empty until connection made
        - self, yes/no?
    - incoming cluster connection properties:
        - peer spid
        - host address (resolved)
        - known hosts, in status message

host name management:
    - large hash table (64k)
    - all known names, aliases, and paths
    - resolve to proxy if known
    

Start cluster:
    - parse known hosts list
        - for each name:
            - create a proxy and open it
            - store name->proxy in hosts hash table
            
Outgoing proxy ready (Connection.Start in amq_proxy.asl):
    - we have peer spid
    - update proxy with spid data
    - if we find an existing proxy with same spid, kill one of them

    ... I'd like to never kill proxies
   

Cluster client ready (Connection.Start-Ok in amq_server_connection.icl):
    - we have peer spid and host address
    - look at all proxies and compare spids
        a. spid matches an already connected proxy
            - we do not need to do anything special
        b. spid is of an unknown server that is talking to us
            - we create a new proxy and connect back to physical address

    ... can we tell if client is one of our known proxies?
    ... it can pass us something, immediately
        ->
            
        
    - create proxy to each known host
        - use async connection, no pinging
        - eventually, connection is made and spid is received
        - remove duplicate proxies, based on spid

    - send status immediately as connected
        - including list of known hosts
        
    - when proxy makes successful connection
        - update spid in 
    - when proxy connects to us
        - use spid to

        
Incoming Cluster.Status
    - parse known hosts list
    - known_host -> peer, via hash table?
        - any new known_hosts, tell cluster controller

- servers know own resolved address
- server can have multiple addresses




problems
    - multiple host names for same server
    - multiple servers configured as root
    -> obvious errors when using configuration data
    -> we can avoid by using only command-line for this
    -> we must also detect...

principles
    - auto-discovery -> auto spid
    - no configurable catastrophes -> no manual override
    - minimal latency -> interconnected
    - expandable -> host discovery mechanism
    - scalable -> WAN bridging protocol
    - autofailover -> primary negotiation
    - simple to use -> no cluster controller
    
    
--------------------------------------------------------------------------

primary partition
    - fully interconnected
    - each server is client of each other server
    - servers can be elected to root
    - servers share primary config data
    OK

secondary partition
    - fully connected to primary partition
    - each server is client of each primary server
    - servers cannot be elected to root
    - servers share primary config data
    OK

root server is most senior primary server
    - takes over when has unanimous vote of active primary servers
    - and active voters are >50% of known primary servers
    - secondary servers do not vote (may change)
    OK

voting procedure
    - each server does its own election
    - based on own SPID and SPIDs of all known peers
        - we expect same election results across all cluster
    OK
    
for best results
    - use 3 or more primary servers
    - all servers have identical cluster configuration
    OK

inbound cluster connection
    - clients logged in as cluster/cluster
    - provide callback and spid at connection time
        - amq_cluster then adds to peer list if not already known
    - may create multiple peer links back to same server
        - duplicates are removed when peer becomes active
        - no active duplicates in peer list
    OK

each cluster controller knows which peer is root
    - when server decides to become root it announces the fact
    - Cluster.Root method to all peers 
    - new peer gets root information via state replication
    OK

method wrapping
    - server method is wrapped as binary bucket
    - placed into cluster-class content
    - primary-> all
    - secondard-> root, (root becomes originator), root-> all
    OK
    
state replication
    - certain AMQP methods are replicated to all servers
        - from primary to all nodes
        - from secondary to all primaries
            - to all nodes except originator via root
    - all such methods form the "cluster state"
    - all primary servers hold list of cluster state methods
    - when new server connects to root
        - receives Cluster.Root message
        - receives full cluster state
        - excluding application messages
    OK

exchange management
    - exchanges are held globally
        - exchange.declare, exchange.delete replicated cluster-wide
        - stateful, replayed onto new servers
    - no guarantee of consistency if application is inconsistent
    OK
    
queue management
    - each server holds all global queues plus own private queues
    - shared queues (exclusive = 0) are held globally
        - queue.declare, queue.delete replicated cluster-wide
        - stateful, replayed onto new servers
    - no guarantee of consistency if application is inconsistent
        - i.e. queue declared differently on different servers
    - queue.purges are replicated iff queue is shared
        - not stateful
    - shared queues are owned by user 0
        - cannot be accessed through exclusive consumer
            -> consider removing exclusive consumer function
    OK

queue bindings
    - queue.binds are replicated if queue is shared
        - stateful, replayed onto new servers
        - shared queues are bound to all exchanges on all nodes
        OK
    - queue.binds are meta-replicated if queue is private
        - cluster.bind 
        - exchange name is same on both machines
        - stateful, replayed onto new servers
    OK

publish message
    - set Cluster-Id property to incoming message
        - [R/B/S]/spid/connection-id/channel_nbr
        - for returning messages to original producer
        - for pushing messages through cluster
    - publish to exchange/binding as normal
    - binding will publish to queues and to remote peers
    binding to queue:
        - if queue shared & cluster enabled & we are not root
            - if message came from application
                - push message to root node
        - else
            - accept message
    binding to peer:
        - if message came from application
            - push message to all bound peers
            - PUSH_ALL
        - if message came from secondary node & we are root
            - push message to all bound secondary peers
            - except where peer->spid == connection->client_spid
            - PUSH_SECONDARY
    OK

cluster vhost
    - string specified in server config and by application
    - specified by application at connection open time
        - Connection.Open.vhost (string)
    - verified by cluster connection manager
    - must also match for intra-cluster connections
    OK

Intra-cluster routing
    "Cluster ID" = spid/connectionid/channelnbr
    - connection->cluster_id = spid/connectionid
    - channel->cluster_id    = spid/connectionid/channelnr
    - content->cluster_id    = spid/connectionid/channelnr
    - consumer->cluster_id   = spid/connectionid/tag
    OK

class.consume, deliver
    - create local consumer as usual
    - if shared queue, broadcast consume method
        - use consumer_tag = connection->cluster_id / consumer_tag
    - delivered messages on proxy connection
        - lookup consumer from method consumer tag
        - deliver onwards to local consumer
    - no-local filtering
        -> content->cluster_id == spid/connectionid/channelnr
        -> consumer->tag == <arbitrary> (direct)
        or consumer->tag == spid/connectionid/<arbitrary> (proxied)
    OK
    
class.cancel
    - cancel local consumer as usual
    - if shared queue, broadcast consume method
        - use cluster_tag as consumer_tag
    OK
        
class.get, get-ok
    - send get to shared queue
    - send get-ok back to connection
        - need some token in the get-ok: cluster-tag
    OK


client connection
    - if cluster enabled but not ready, wait
        - not ready = no root server defined yet
        - hold onto client connection...
    - if cluster disabled
        - pause client activity
        - client application can timeout if bored
    - use list of server:port names from application
        - try each host in sequence, drop any that don't reply
        - connect to first host that responds
    - if we get Connection.Redirect
        - connect to hostname and then known-hosts in order
    OK
    
    
    
    - add force option to C API
    - cluster needs to balance clients over all servers...
        - reconnect to server with lowest usage




Clarify in doc:
    - client can send synchronous methods without waiting for responses
    - responses MAY come out of order...
        - e.g. queue.purge, basic.get, etc.
    - do we add a response token to all synchronous methods?
        - carried in replies?
    - shortstr, allows arbitrary encodings

Change requests
    - if we have multiple vhosts
        - create a proxy connection per vhost
    POSTPONE
    - general technique for proxying AMQP methods
        - proxy method with/without content?
            - requires hierarchical content support
        - document in AMQP specifications
    IN PROGRESS

    - implement heartbeating between brokers
        - information, per broker:
            - connections, messages, allocation
            - exchanges, queues, bindings, peers
            - primary, root
    DONE


OpenAMQ change requests (to Jira)
    - sigusr1 -> dump state to console
    - sigusr2 -> dump 1line state to console
    - implement limit on returned messages per connection
    - build command-line console in kernel (generated)

    - if returned,
        - if origin-spid is known
            - return to peer
        - else
            - return to root
    - on delivery to consumer
        - remove origin-spid header




    ... how to return messages if no immediate consumers?
        - need to add original connection & channel number to message
        - need to process incoming returns

console issues
    - replicate configuration across cluster
    - enable/disable root on specific server


Figures

We can measure 'Internal Switching Rate' (ISR) from a faucet into sinks
(queues that have no consumers, so discard their messages).

With a single broker we can switch into Q queues, giving a ratio of Q,
and a tentative capacity of Q.10KM/s. When Q is 16 this gives us a
tentative ISR of 160KM/s, or 4.61TM/wd (per working day of 8 hours).

Per broker, the value of Q can reasonably go up to 256 or higher. The
XSR will depend on the network capacity of a broker but will always be
limited to the ISR. AMQ is designed for values of Q as high as 10,000.

With multiple brokers B fanning out from a root broker, we calculate the
ISR ratio as B.Q. With B = 6, and Q = 16, we have a tentative ISR of
960KM/s, or 27.65TM/wd.

With larger values for Q, we get extremely high ISRs and the cluster
efficiency approaches 100%. However these ISR measurements do not
translate into effective XSRs due to impossibiity of shifting that
amount of data across broker network links.

Maximum practical value for Q is 12, after which the XSR cannot keep up
with ISR, and this assumes 1.0Gb of bandwidth per broker.

Higher values of Q are feasible if message selectors are used to prune
messages arriving in sinks.

Assuming a maximum Q of 12, we get higher XSRs by increasing B and the
number of cluster peers. The cluster size will be 1 + B, and the limit
of B is identical to the limit of Q, namely 12. The maximum XSR in this
case (B = 12, Q = 12) is 1.44 MM/s and requires a cluster of 13 nodes.

Assuming a less efficient network between brokers and external clients,
with 30% of GB capacity, the maximum value for Q decreases to 4 and the
XSR correspondingly (480 KM/s). If the internal cluster network is also
less efficient, we arrive at B = Q = 4.

A reasonable configuration for testing cluster ISR is:

 - B = 7, Q = 12, cluster size = 8 plus one external faucet.

A reasonable configuration for testing cluster XSR is:

 - B = 3, Q = 4, cluster size = 4 plus one external faucet and
   12 external clients.

Note that the XSR/ISR estimates are based on a message size of 1000 bytes
and a broker capacity of 10 KM/s.  More realistic figures would be
message size of 250 bytes and capacity of 25 KM/s.


