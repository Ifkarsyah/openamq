gdl
    title     = OpenAMQ Clustering
    subtitle  = A Redundant Array of Inexpensive Servers
    product   = OpenAMQ
    author    = iMatix Corporation <amq@imatix.com>
    date      = 2005/10/17
    copyright = Copyright (c) 2004-2005 JPMorgan
    version   = 1.0c
end gdl

Redundant Array of Inexpensive Servers
**************************************

Statement of Goals
==================

RAIS is a proposal for clustering OpenAMQ servers.  RAIS was designed by
Pieter Hintjens after discussions with Mark Atwell and Steve Connelly.

The goals of the RAIS design are:

 1. To provide arbitrarily scalable performance through the addition of
    cheap hardware.

 2. To provide high reliability through the use of redundancy.

 3. To support all AMQP scenarios, including transient pub-sub, persistent
    request-response, point-to-point queues, file-transfer, streaming etc.

 4. To implement clustering with minimum impact on the AMQ server and in
    such a way that clustering of different AMQ server implementations is
    possible.

Our goal in a transient pub-sub scenario is to achieve a sustained traffic
rate of 1M messages per second, or 500K reliable messages per second on a
server cluster costing under EUR 10,000.

RAIS is inspired by three things:

 1. The original concepts of RAID storage, namely the assembly of modest
    components into a high-performance array.

 2. The design approach of Google's networks, which are based on cheap boxes
    interconnected by smart software.

 3. The desire to find a way to scale OpenAMQ servers that does not depend
    on optimisation, which always reaches limits.

General Principles
==================

Overall Architecture
--------------------

The architecture consists of:

 - A set of information publishers (P).
 - A set of information subscribers (S).
 - A set of OpenAMQ servers (A).

Each publisher maintains a persistent connection to each server.  Each subscriber
likewise maintains a persistent connection to each server.  The servers do not
communicate with each other but remain connected to the entire set of publishers
and subscribers:

    P   P   P
     \ / \ /
      A   A
     / \ / \
    S   S   S

Each P and each S is connected to each A, though most of these connections are
not shown to keep the diagram simple.

The state on any server A consists of the subscription information needed to
route messages to the appropriate subscriber.  In AMQP terms this consists of
queues, bindings, and consumers.

If a new server joins the network, every publisher connects to the new server,
and every subscriber connects to the new server and recreates its subscription
information:

    P   P   P
     \ / \ / \
      A   A   A
     / \ / \ /<-- declare queues, bind queues, define consumers
    S   S   S

We avoid the complexity of synchronising the state of different servers by
eliminating inter-server communication from the architecture.  Servers do not
talk to each other.

Thus in its stable state, every server A has an exact mirror of the subscription
state of every other A.  We will now explore the different ways we can exploit
this.

Performance Through Striping
----------------------------

To gain performance, we "stripe" the messages.  This is exactly the same technique
as used in RAID: each publisher writes data a round-robin fashion to the different
servers it is connected to.

Since each consumer is connected to every server, consumers will receive the same
messages.  Ordering may not be maintained, since messages flow through different
paths.

Reliability Through Mirroring
-----------------------------

To gain reliability, we "mirror" the messages.  This means: sending the same
message redundantly to more than one server, and rejecting duplicate messages
on receipt.

If the risk of a server crash is 1/R, then a double-redundancy (sending each
message to 2 servers) will reduce the risk of a server crash to 1/(R*R).

Combined Striping and Mirroring
-------------------------------

Given a cluster of more than 2 servers, we can combine striping and mirroring
to achieve both reliability and performance.

Implementation Proposal
=======================

RAIS Implementation
-------------------

RAIS depends on client-side intelligence.  We can place this intelligence
in the client application, in the client API, or elsewhere.  The first two
solutions present some difficulties, because RAIS is potentially a complex
problem and it is not a good idea to mix this complexity with existing
applications (they will do it wrong or in different ways), nor with the
existing APIs (we would need multiple implementations, one for each API
technology - C, JMS, Perl, etc.).

So, we propose to implement RAIS as a proxy server.  A RAIS proxy (R)
runs on every client system, and maps a single virtual connection into
the full set of connections to the RAIS cluster:

    P   P   P
    R   R   R
     \ / \ /
      A   A
     / \ / \
    R   R   R
    S   S   S

Applications connect to the RAIS port on 127.0.0.1, and conduct a normal
single-connection AMQP dialogue.  The RAIS proxy does the following:

 1. It caches security information so that it can connect to all
    servers using identical parameters.

 2. It caches all subscription information (queue.declare, queue.bind,
    jms.consume, channel.flow, etc. methods) so that it can recreate
    this on all servers identically.

 3. It applies the desired striping and mirroring of messages on all
    jms.publish methods.

 4. It caches received messages (jms.deliver) and eliminates duplicates
    coming from multiple servers.

The RAIS proxy holds its data in memory only, since if the client
application ends, the RAIS proxy can also disconnect and end.

Browsing Queues
---------------

Compared to a real single server, a RAIS cluster does not hold messages on
a single queue but rather holds them (usually redundantly) on several queues
that have the same name but are in different locations.

To implement a "browse" command that selects individual messages from a queue,
we must:

 - Send the browse command to all servers.
 - Wait for all replies and collate them.
 - Present a single coherent response to the application.

Cluster Synchronisation
-----------------------

We do not care about the set of publishers on the cluster - adding or
removing a publisher has no impact on the way messages are routed.

When a new subscriber joins the cluster, it will register with each
server one-by-one.  It can send its registration commands (queue.create,
queue.bind, jms.consume) in batches to each server, but there is still
a significant window during which some servers will not be ready while
others will be.

During this window, the subscriber will not be guaranteed of getting
all its messages correctly, as messages that are sent to an out-of-date
server will not reach it.

A new subscriber - or rather, the RAIS proxy that is acting for it -
can monitor the cluster to know when it is "ready", and the incoming
message stream is reliable enough to pass to the application.

We propose the following mechanism:

 1. Each server will publish metadata about new consumers on a standard
    topic exchange.

 2. Each RAIS proxy will subscribe to this information steam and thus
    be able to monitor the state of each server.

 3. Subscriber-side RAIS proxies will use this state information to
    decide when the stream of data from the cluster is stable enough to
    pass to the application.

We can use a standard topic exchange, e.g. amq.system.rais.  The
presence or absence of this exchange will tell RAIS proxies whether
the server supports RAIS synchronisation or not.

Clustered Point-to-Point
------------------------

Pub-sub is trivial to implement in RAIS since each queue has a single
consumer, and fan-out is done by creating multiple queues, one for each
subscriber.  Thus, each queue can unambiguously send its messages to
its consuming client without problems (the only effect being that
messages may arrive out of sequence as they travel different paths).

But AMQ also allows point-to-point communications, using shared queues,
which is essential for work-load distribution, and the classic scenario
for message queues.

Given the description of RAIS so far, this presents us with a problem.
If a message is sent to a queue that is mirrored on two or more cluster
servers, for reliability, how do we ensure consistent delivery?  A
quick sketch of messages flowing arbitrarily across servers in a cluster
shows that it is not at all stable.

Summary of the Problem
......................

The main issues can be boiled down to three points:

 1. Ensuring that a specific message, passing through two or more
    mirrored queues in parallel, will always reach the same end-point.

 2. Synchronising changes to consumers for queue so that all mirrored
    queues have the same set of active consumers at all instants.

 3. Allowing the mirrored distribution of consumer-queue state such
    as prefetch and transactions.

We have examined several plausible solutions to this problem.  There
seems to be only one that works.  The key insight is this: when we mirror
a queue across multiple servers, we must keep the state of each instance
of the queue identical.  That means: the exact same set of consumers and
the exact same set of messages.  If the queues do not have the same state,
there is no guarantee that a message that has been mirrored to two or
more queues will be dispatched in the same way, to the same consumer.

The General Solution
.....................

The solution is this: when doing point-to-point publishing, the publisher
must send the message to *all* servers, so that it arrives equally on all
queues, and each queue must perform an identical calculation - taking into
account the queue and consumer state - and thus send the message to the
same consumer.  And this must remain stable even when consumers join and
leave the cluster.

Clearly when we speak of large clusters, sending every message to every
server would create significant extra network traffic.

Sub-Clusters Through Virtual Hosts
..................................

We can create sub-clusters using the AMQ concept of "virtual host", as
follows:

 - Each server in the cluster would support one or more virtual hosts,
   creating an overlapping set of virtual hosts, each running on one
   or more servers.

 - When a client (consumer or publisher) connects to the cluster and
   then to a specific virtual host, via its RAIS proxy, the proxy will
   successfully connect only to those servers that implement the desired
   virtual host.  This creates the sub-cluster.

We can now use the cluster to gain throughput by separating applications,
sets of queues, or even individual queues into different virtual hosts.

At the same time, by tuning the number of instances of each virtual
host (the number of servers that support it), we can tune the redundancy
level of the network.  Since a single server can support an arbitrary
number of virtual hosts, sub-clustering will be easy to manage.

The End-Points
..............

The publishing proxy needs to know whether it is working in pub-sub or
point-to-point mode.  In the former case it sends to one or more servers
depending on the reliability vs. performance tuning.  In the latter case
it sends to all servers.

The consuming proxy is connected to all queues, so can expect to receive
a message exactly one time per connected server.  It can process and
acknowledge the first instance of the message that arrives, and
acknowledge and discard the remaining messages that arrive.

The Vanishing Consumer Problem
..............................

What happens if a consumer crashes and leaves the cluster while it has
processed one instance of a message but not others?  We now risk getting
inconsistency between the queues.

 - process first, ack last
 - acks may get lost
    - cross-ack between queues

 -


- clustered console
- no publishing while consumer set is changing
- flush from publishers down
- controller tells all servers, "prepare"
- all servers tell all publishers, "prepare"
- all publishers flush outgoing messages
- any new messages to publish are queued in publisher
- all publishers signal all servers, "ready"
- servers collect all "ready" responses, signal controller "ready"
- controller creates new consumer on each server
- controller tells all servers, "clear"
- all servers tell all publishers, "clear"



The simple way is to not send messages to servers that are "out of
date" from the point of view of publishers.  We have already described
a mechanism by which RAIS proxies can track the state of each server.
We can use this mechanism in publisher-side RAIS proxies to detect
out-of-date servers.

There is a window for error here - a consumer may have been registered
with a server but the server may not yet have warned the RAIS proxies
that there is a state change.

To be totally robust, we must pause each queue that receives a new
consumer (a paused queue will accept new messages but not deliver
them to consumers), and then resume the paused queues once every
server has confirmed that it has registered the new consumer.

A more complete solution needs a two-level handshake between the
servers and a selected RAIS proxy that we will designate the "cluster
manager".  This works as follows:

 - A new subscriber joins a server, creating a consumer on that
   server.

 - The server marks the consumer as "inactive" and publishes its
   change of state to the cluster manager.

 - The server pauses its queue, so that new incoming messages are
   not routed to consumers on this queue.  Other queues continue
   to work unaffected.

 - This process repeats as the subscriber joins each server in turn.

 - The cluster manager collect these reports in turn and collates
   the results.

 - When all servers report the same status, the cluster manager
   sends all servers a "accept consumer" message.

 - Given this message, the servers activate the new consumer and
   then reply with an acknowledgement.

 - The cluster manager collects these acknowledgements and when it
   has them all, it sends all servers a "resume queue" message.

 - The servers, on receiving this message, start sending messages
   out to all consumers on the queue.

This model guarantees that the set of consumers is identical across
all servers before messages are sent out of a queue.

[Author's note: in fact the above model still allows broken mirrors:
queues are paused one by one, leaving the chance for the same message
to be sent to two different consumers.  Needs more work...]

We can choose the cluster manager in several ways.  Our preference
is to allow each server to choose several cluster managers from the
set of publishing RAIS proxies known to it, using some algorithm
that gurantees that there will be an overlap of at least one.


Problem Statement

 - Message M is sent to two instances of queue, Q1 and Q2.
 - A new consumer C is attaching to both queues.  It attaches to Q1.
 - M now arrives in both queues, and both queues dispatch it.
 - New consumer C now attaches to Q2.

System:

 - RAIS proxy that is doing the consume acts as "sentinel".
 - The sentinel sends all servers a "prepare" command specifying the
   queue.
 - Each server responds with an acknowledgment.
 - At this point, arbitrary messages have been passed through the queues.
 -

 - sentinel can be proxy making consumer
 - how do we synchronise same message on two servers...
    - message has to do out before queue is paused
    - can we introduce syncpoints?

 - proxy wants to consume
    - asks all queues to prepare
    -

queues know last message
    - 

    Q1:    M1  *  M2 *   M4
    Q2:    M1  *  M3     M4
    Q3:    M2  *  M3
    
 
Goal: to ensure that all instances of queue have same set of consumers
while messages are exchanged.

Note that queues may be receiving messages arbitrarily, i.e. message N
may arrive at queues Q1 and Q2 with consirable lag.  [need a diagram to
demonstrate the problem windows].



Method: pause and flush queues with existing consumers, allow all queues to get
new consumers, 

    - RAIS proxy that is making the consumer


Cluster Maintenance
-------------------

There are (at least) two ways of maintaining the cluster. One is to
define the set of servers (their IP address and port) manually so
that all RAIS proxies share this information.  The second is to use
multicast techniques to detect when new servers are added to the
cluster.

It should also be possible to use AMQP to distribute information about
the cluster.

Server Configuration
--------------------

As the number of servers increase, it is clearly more work to manage
these.  We propose several techniques to minimise this work:

 1. To use static configuration rather than on-line operational
    control.  I.e. rather than configure servers interactively, the
    servers should be configured from configuration files.

 2. To use network booted RAM disks to load the server operating system,
    software, and configuration data.  We have used this successfully
    in high-reliability projects.  It does require a separate boot
    server but this can itself be CD-booted, and is only a point of
    failure when the cluster changes.

 3. To make the RAIS proxy aware of the operational control process so
    that operational control commands - such as watches - can be applied
    to all servers in the cluster automatically.

Cost and Planning
=================

Costs
-----

A very rough estimate of the cost of this work is:

 - RAIS proxy alpha: 10 days' work.
 - RAIS proxy beta: 15 days' work.
 - RAIS synchronisation support in server: 5 days' work.
 - RAIS integration with operational control: 10 days' work.
 - Add RAIS support to JMS and C APIs: 2 days' work.
 - Stability and load testing: 15 days' work.

It should be possible to build a working RAIS implementation with
2-3 months of work.

Proof-of-Concept Proposal
-------------------------

We estimate that a single CPU is capable of transferring 40K messages
per second given a very fast network card.  This figure comes from tests
that have shown rates of 50K messages per second on loopback networks.

The following architecture should demonstrate the capacity of the RAIS
cluster:

 - One fast server acting as publisher.
 - A cluster of 4 inexpensive servers, providing fast CPU, fast NIC,
   and with minimum RAM, no storage, no case, etc.  Estimated cost:
   EUR 400 per server.
 - A set of 4 workstations acting as sink clients (to accept and
   discard messages).  Each workstation would run one client process.
 - A fast switch interconnecting these systems, all using gigabit
   networking.

Using full fan-out (all subscribers receive all data), the volumes we
expect to see are:

 - 40K messages per second outgoing from each publisher.
 - 10K messages per second incoming to each server (40K in total).
 - 40K messages per second outgoing from each server (160K in total).
 - 40K messages per second incoming to each client (160K in total).

Giving us a total cluster throughput of 200K messages per second for
a cluster cost of EUR 1,600.  This should be scalable to 1M messages
per second, using an appropriate network organisation, for well under
EUR 10,000.

Proof-of-Concept Demonstration
------------------------------

For an effective demonstration, we would want to see:

 - A sustained message transfer rate that is proportional to the size
   of the cluster.
 - Cluster stability as clients and servers are removed and added on
   the fly.
 - Different messaging scenarios, including pub-sub with private
   subscription queues, and request-response with shared work queues.
