gdl
    title     = AMQ R.A.I.S. Clustering
    subtitle  = A Redundant Array of Inexpensive Servers
    product   = OpenAMQ
    author    = iMatix Corporation <amq@imatix.com>
    date      = 2005/10/12
    copyright = Copyright (c) 2004-2005 JPMorgan
    version   = 1.0a
end gdl

Redundant Array of Inexpensive Servers
**************************************

Statement of Goals
==================

RAIS is a proposal for clustering OpenAMQ servers.  RAIS was designed by
Pieter Hintjens after discussions with Mark Atwell and Steve Connelly.

The goals of the RAIS proposal are:

 1. To provide arbitrarily scalable performance through the addition of
    cheap hardware.

 2. To provide high reliability through the use of redundancy.

 3. To support all AMQP scenarios, including transient pub-sub, persistent
    request-response, point-to-point queues, file-transfer, streaming etc.

 4. To implement clustering with minimum impact on the AMQ server and in
    such a way that clustering of different AMQ server implementations is
    possible.

Our goal in a transient pub-sub scenario is to achieve a sustained traffic
rate of 1M messages per second, or 500K reliable messages per second on a
server cluster costing under EUR 10,000.

RAIS is inspired by three things:

 1. The original concepts of RAID storage.

 2. The design approach of Google's networks, which are based on cheap boxes
    interconnected by smart software.

 3. The desire to find a way to scale OpenAMQ servers that does not depend
    on optimisation.

General Principles
==================

Overall Architecture
--------------------

The architecture consists of:

 - A set of information publishers (P).
 - A set of information subscribers (S).
 - A set of OpenAMQ servers (A).

Each publisher maintains a persistent connection to each server.  Each subscriber
likewise maintains a persistent connection to each server.  The servers do not
communicate with each other but remain connected to the entire set of publishers
and subscribers:

    P   P   P
     \ / \ /
      A   A
     / \ / \
    S   S   S

The state on any server A consists of the subscription information needed to
route messages to the appropriate subscriber.  In AMQP terms this consists of
queues, bindings, and consumers.

If a new server joins the network, every publisher connects to the new server,
and every subscriber connects to the new server and recreates its subscription
information:

    P   P   P
     \ / \ / \
      A   A   A
     / \ / \ /<-- declare queues, bind queues, define consumers
    S   S   S

We avoid the complexity of synchronising the state of different servers by
eliminating inter-server communication from the architecture.  Servers do not
talk to each other.

Thus in its stable state, every server A has an exact mirror of the subscription
state of every other A.  We will now explore the different ways we can exploit
this.

Performance Through Striping
----------------------------

To gain performance, we "stripe" the messages.  This is exactly the same technique
as used in RAID: each publisher writes data a round-robin fashion to the different
servers it is connected to.

Since each consumer is connected to every server, consumers will receive the same
messages.  Ordering may not be maintained, since messages flow through different
paths.

Reliability Through Mirroring
-----------------------------

To gain reliability, we "mirror" the messages.  This means: sending the same
message redundantly to more than one server, and rejecting duplicate messages
on receipt.

If the risk of a server crash is 1/R, then a double-redundancy (sending each
message to 2 servers) will reduce the risk of a server crash to 1/(R*R).

Combined Striping and Mirroring
-------------------------------

Given a cluster of more than 2 servers, we can combine striping and mirroring
to achieve both reliability and performance.

Implementation Proposal
=======================

RAIS Implementation
-------------------

RAIS depends on client-side intelligence.  We can place this intelligence
in the client application, in the client API, or elsewhere.  The first two
solutions present some difficulties, because RAIS is potentially a complex
problem and it is not a good idea to mix this complexity with existing
applications (they will do it wrong or in different ways), nor with the
existing APIs (we would need multiple implementations, one for each API
technology - C, JMS, Perl, etc.).

So, we propose to implement RAIS as a proxy server.  A RAIS proxy (R)
runs on every client system, and maps a single virtual connection into
the full set of connections to the RAIS cluster:

    P   P   P
    R   R   R
     \ / \ /
      A   A
     / \ / \
    R   R   R
    S   S   S

Applications connect to the RAIS port on 127.0.0.1, and conduct a normal
single-connection AMQP dialogue.  The RAIS proxy does the following:

 1. It caches security information so that it can connect to all
    servers using identical parameters.

 2. It caches all subscription information (queue.declare, queue.bind,
    jms.consume, channel.flow, etc. methods) so that it can recreate
    this on all servers identically.

 3. It applies the desired striping and mirroring of messages on all
    jms.publish methods.

 4. It caches received messages (jms.deliver) and eliminates duplicates
    coming from multiple servers.

The RAIS proxy holds its data in memory only, since if the client
application ends, the RAIS proxy can also disconnect and end.

Browsing Queues
---------------

Compared to a real single server, a RAIS cluster does not hold messages on
a single queue but rather holds them (usually redundantly) on several queues
that have the same name but are in different locations.

To implement a "browse" command that selects individual messages from a queue,
we must:

 - Send the browse command to all servers.
 - Wait for all replies and collate them.
 - Present a single coherent response to the application.

Cluster Synchronisation
-----------------------

We do not care about the set of publishers on the cluster - adding or
removing a publisher has no impact on the way messages are routed.

When a new subscriber joins the cluster, it will register with each
server one-by-one.  It can send its registration commands (queue.create,
queue.bind, jms.consume) in batches to each server, but there is still
a significant window during which some servers will not be ready while
others will be.

During this window, the subscriber will not be guaranteed of getting
all its messages correctly, as messages that are sent to an out-of-date
server will not reach it.

A new subscriber - or rather, the RAIS proxy that is acting for it -
can monitor the cluster to know when it is "ready", and the incoming
message stream is reliable enough to pass to the application.

We propose the following mechanism:

 1. Each server will publish metadata about new consumers on a standard
    topic exchange.
 2. Each RAIS proxy will subscribe to this information steam and thus
    be able to monitor the state of each server.
 3. Subscriber-side RAIS proxies will use this state information to
    decide when the stream of data from the cluster is stable enough to
    pass to the application.

We can use a standard topic exchange, e.g. amq.system.rais.  The
presence or absence of this exchange will tell RAIS proxies whether
the server supports RAIS synchronisation or not.

Clustered Shared Queues
-----------------------

Pub-sub is trivial to implement in RAIS since each queue has a single
consumer, and fan-out is done by creating multiple queues, one for each
subscriber.  Thus, each queue can unambiguously send its messages to
its consuming client without problems (the only effect being that
messages may arrive out of sequence as they travel different paths).

But AMQ also allows shared queues, essential for work-load distribution,
the main way of using standard message queues.

Given the description of RAIS so far, this presents us with a problem.
If a message is sent to two queues - on two cluster servers - how do we
ensure the two messages are delivered to the same client application?

Consider the following scenario:

 - We have three servers, S1, S2, S3.
 - We have two consumers, C1 and C2, each connected to a queue Q
   that has one instance (Q1, Q2, Q3) on each server.
 - A publisher sends a series of messages, M1, M2, M3, M4...
 - We use mirroring for reliability, and send the message to two
   servers each time.

The queues will receive messages as follows (assuming the publishers
use simple a round-robin):

    Q1: M1, M2, M4
    Q2: M1, M3, M4
    Q3: M2, M3

Our ideal routing of messages to consumers would be:

    C1: M1, M3
    C2: M2, M4

A duplicated message sent from a clustered queue (a queue with the same
name held on two different servers) must be sent to the same consumer,
whatever the messages that were sent before it from the same queue.

We can achieve this by using a stateless hashing function to select the
consumer for a given message, rather than using a round-robin technique.
Given a specific message, and a specific set of consumers, a stateless
hashing function will always send the message to the same consumer.

Our second challenge is to stop messages from arriving in queues that
do not have the same set of consumers.  We see two ways of doing this,
a simple way and a more complex but probably more correct way.

The simple way is to not send messages to servers that are "out of
date" from the point of view of publishers.  We have already described
a mechanism by which RAIS proxies can track the state of each server.
We can use this mechanism in publisher-side RAIS proxies to detect
out-of-date servers.

There is a window for error here - a consumer may have been registered
with a server but the server may not yet have warned the RAIS proxies
that there is a state change.

A more complete solution needs a two-level handshake between the
servers and a selected RAIS proxy that we will designate the "cluster
manager".  This works as follows:

 - A new subscriber joins a server, creating a consumer on that
   server.
 - The server marks the consumer as "inactive" and publishes its
   change of state to the cluster manager.
 - The server pauses its queue, so that new incoming messages are
   not routed to consumers on this queue.  Other queues continue
   to work unaffected.
 - This process repeats as the subscriber joins each server in turn.
 - The cluster manager collect these reports in turn and collates
   the results.
 - When all servers report the same status, the cluster manager
   sends all servers a "accept consumer" message.
 - Given this message, the servers activate the new consumer and
   then reply with an acknowledgement.
 - The cluster manager collects these acknowledgements and when it
   has them all, it sends all servers a "resume queue" message.
 - The servers, on receiving this message, start sending messages
   out to all consumers on the queue.

This model guarantees that the set of consumers is identical across
all servers before messages are sent out of a queue.

We can choose the cluster manager in several ways.  Our preference
is to allow each server to choose several cluster managers from the
set of publishing RAIS proxies known to it, using some algorithm
that gurantees that there will be an overlap of at least one.

Cluster Maintenance
-------------------

There are (at least) two ways of maintaining the cluster. One is to
define the set of servers (their IP address and port) manually so
that all RAIS proxies share this information.  The second is to use
multicast techniques to detect when new servers are added to the
cluster.

Server Configuration
--------------------

As the number of servers increase, it is clearly more work to manage
these.  We propose several techniques to minimise this work:

 1. To use static configuration rather than on-line operational
    control.  I.e. rather than configure servers interactively, the
    servers should be configured from configuration files.

 2. To use network booted RAM disks to load the server operating system,
    software, and configuration data.  We have used this successfully
    in high-reliability projects.  It does require a separate boot
    server but this can itself be CD-booted, and is only a point of
    failure when the cluster changes.

 3. To make the RAIS proxy can be aware of the operational control
    process so that operational control commands - such as alerts -
    can be applied to all servers in the cluster automatically.

Cost and Planning
=================

Costs
-----

A very rough estimate of the cost of this work is:

 - RAIS proxy alpha: 10 days' work.
 - RAIS proxy beta: 15 days' work.
 - RAIS integration with operational control: 10 days' work.
 - Add RAIS support to JMS and C APIs: 2 days' work.
 - Stability and load testing: 15 days' work.

It should be possible to build a working RAIS implementation with
2-3 months of work.

Proof-of-Concept Proposal
-------------------------

We estimate that a single CPU is capable of transferring 40K messages
per second given a very fast network card.  This figure comes from tests
that have shown rates of 50K messages per second on loopback networks.

The following architecture should demonstrate the capacity of the RAIS
cluster:

 - One fast server acting as publisher.
 - A cluster of 4 inexpensive servers, providing fast CPU, fast NIC,
   and with minimum RAM, no storage, no case, etc.  Estimated cost:
   EUR 400 per server.
 - A set of 4 workstations acting as sink clients (to accept and
   discard messages).  Each workstation would run one client process.
 - A fast switch interconnecting these systems, all using gigabit
   networking.

Using full fan-out (all subscribers receive all data), the volumes we
expect to see are:

 - 40K messages per second outgoing from each publisher.
 - 10K messages per second incoming to each server (40K in total).
 - 40K messages per second outgoing from each server (160K in total).
 - 40K messages per second incoming to each client (160K in total).

Giving us a total cluster throughput of 200K messages per second for
a cluster cost of EUR 1,600.  This should be scalable to 1M messages
per second, using an appropriate network organisation, for well under
EUR 10,000.

Proof-of-Concept Demonstration
------------------------------

For an effective demonstration, we would want to see:

 - A sustained message transfer rate that is proportional to the size
   of the cluster.
 - Cluster stability as clients and servers are removed and added on
   the fly.
 - Different messaging scenarios, including pub-sub with private
   subscription queues, and request-response with shared work queues.
