gdl
    title     = OpenAMQ Clustering
    subtitle  = Providing Reliability and Scalability
    product   = OpenAMQ
    author    = iMatix Corporation <openamq@imatix.com>
    date      = 2006/01/04
    copyright = Copyright (c) 2004-2006 JPMorgan
    version   = 1.2a1
end gdl

General Discussion
******************

What is "Clustering"?
=====================

We use the term "cluster" to mean a set of brokers that are tied
together in such a way as to create a single virtual broker:

           +------------------+------------------+
           |                  |                  |
    +------+-----+     +------+-----+     +------+-----+
    |            |     |            |     |            |
    |   Broker   |     |   Broker   |     |   Broker   |
    |            |     |            |     |            |
    +------+-----+     +------+-----+     +------+-----+
           |                  |                  |
           +------------------+------------------+
                              | Cluster
                              |
    --------------------------+-------------------------
                              |
                       +------+------+
                       |             |
                       | Application |
                       |             |
                       +-------------+

Note that the brokers may be physically distant.  We do not use the
term to describe sets of interconnected brokers that serve different
sets of applications, e.g.:

        +---+                      +---+
      +-+   +-+                  +-+   +-+
      |       |      Bridge      |       |
      |       | ---------------- |       |
      +-+   +-+                  +-+   +-+
        +-+-+                      +-+-+
          |                          |
          |                          |
    +-----+-----+              +-----+-----+
    |           |              |           |
    |Application|              |Application|
    |           |              |           |
    +-----------+              +-----------+
      New York                     Tokyo

The correct term for this is "bridging", something that is done by
specialised applications.  See the document entitled "AMQ Bridging".

Why Cluster?
============

The main reasons for clustering are:

1. Reliability: using redundant brokers so that if one fails, another
   can take its place.  Reliability is principally for point-to-point
   messages, the classic "queue service" scenario.

2. Scalability: using multiple brokers so that work can be distributed
   over more boxes and/or networks.  Scalability is principally for
   transient publish/subscribe messages, the classic "topic service"
   scenario.

Since AMQ allows applications to mix the classic concepts of queue and
topic into arbitrary messaging models, AMQ clustering must support both
reliability and scalability, though these are somewhat different
problems.

Features of AMQ Clustering
==========================

We have some specific design goals for AMQ clustering:

1. Simple to configure, administrater, and use in applications.

2. Simplicity of implementation, except where the system becomes more
   complex to use.

3. Ability to scale to any size.

4. Ability to use arbitrary storage systems (SAN, RAID, IDE).

5. A single solution that provides both scalability and reliability,
   albeit targetted towards specific application scenarios.

Terminology
===========

We use the term "persistent" to mean data that is saved on disk, and
survives broker restarts. This is contrasted with "transient", which
means data held only in memory, and lost if the broker restarts. We use
the term "reliable" to mean persistent data that is mirrored in some way
to make it robust against disk failures.

Levels of Reliability
=====================

We can define four levels of message reliability, each providing a
different cost/reliability trade-off:

 - Fully transient: the message is held only in one memory and is
   lost if that memory is reset.

 - Reliable transient: the message is replicated to a second memory
   and is lost only if both memories are reset.

 - Persistent: the message is saved to a single disk system and is
   lost if that disk system is damaged.

 - Reliable persistent: the message is saved to two distinct disk
   systems and is lost only if both disk systems are damaged.

Transactions vs. Messages
=========================

Although AMQ is a message-oriented middleware system, it is useful to
consider reliability in terms of "transactions". A transaction consists
of a set of actions that imply change to the broker state, and
specifically, the set of messages held by a queue:

 - The simplest transactional model wraps every method in a transaction.
   We call this an "automatic transaction".

 - The classic transaction model wraps a set of methods in a transaction.

 - The distributed transaction model coordinates transactions across a
   set of brokers using a distributed transaction coordinator.  This is
   more complex than we want to implement at present.

Transactions can cover any mix of persistent, or transient messages, and
durable or temporary queues.

We do not make any a-priori distinction between persistent and transient
messages, except that they are stored on more or less reliable media.
We might have several types of persistent messages, for instance.

General Solution for Reliability
================================

We consider the problem of storing persistent transactions in such a
way that a broker or disk system failure does not cause loss of data.
This is the classic "reliable point-to-point messaging" scenario.

The cluster consists of two brokers, one acting as "root" and one as
backup. In normal operation, all clients connect to the root broker,
as shown by this diagram:

        +-----------+        +-----------+
        |           |        |           |
        |   Root    +--------+  Backup   |
        |  Broker   +--------+  Broker   |
        |           |        |           |
        +----/+\----+        +-----------+
           // | \\
          /   |   \\
         /    |     \
       //     |      \\
     //-\    /+-\    /-\\
    |    |  |    |  |    |
    |    |  |    |  |    |
     \--/    \--/    \--/

      Client Applications

Each broker maintains its own storage, and the root broker replicates
transactions to the backup broker as follows:

        -----                -----
      //     \\            //     \\
     | Disk 1  |          | Disk 2  |
      \\     //            \\     //
        --+--                --+--
          | ||||||             | ||||||
          |  B                 |  D
    +-----+-----+        +-----+-----+
    |           |  C     |           |
    |   Root    +-||||||-+  Backup   |
    |  Broker   +--------+  Broker   |
    |           |     E  |           |
    +-----+-----+        +-----------+
          | ||||||
         F|  A
          |
         /+-\
        |    |
        |    |
         \--/
        Client

 - (A) The client prepares a transaction (publishes messages and
   acknowledges received messages) on the root broker.  The
   client then does a COMMIT.

 - (B) The root broker writes this transaction to disk.  It may
   or may not flush the disk, this is a matter of optimisation.

 - (C) The root broker passes the transaction to the backup
   broker.

 - (D) The backup broker writes this transaction to disk.  Again,
   it may or may not flush the disk depending on the degree of
   reliability wanted.

 - (E) The backup broker confirms that the transaction has been
   recorded safely.

 - (F) The root broker confirms to the client application that
   the transacton has been completed.

If the root broker fails, the client can detect this and switch
to the backup broker:

           -----                -----
         //     \\            //     \\
        | Disk 1  |          | Disk 2  |
         \\     //            \\     //
           --+--                --+--
             |                    |
          ---+---                 |
      / //-------\\ \       +-----+-----+
     | |           | |      |           |
    |  |   Root    |  |     |  Backup   |
    |  |  Broker   |  |     |  Broker   |
     | |           | |      |           |
      \ \\\-----/// /       +-----------+
           -----            ----
                        ----
                    ----
                ----
            /--\
           |    |
           |    |
            \--/
           Client

General Solution for Scalabilty
===============================

We consider the problem of handling very large numbers of clients (100k
or more) that consume data from a small number of high-volume
publishers. This is the classic "transient publish-subscribe scenario".

This scenario has several particular features:

 - It does not use persistent transactions. This means
   there is no requirement to replicate transactions.

 - It has a strong fan-out ratio from publishers to subscribers.
   This means the same message can be sent to very many end-points.

We in fact have two kinds of client application, publishers and
subscribers, and we can draw the cluster like this, showing a
set publisher and several sets of subscribers:

                       Publishers
                          ---
                        //   \\
                       |       |
                       |       |
                        \\   //
                          -+-
                           |
          +----------------+-----------------+
          |                |                 |
    +-----+-----+    +-----+-----+     +-----+-----+
    |           |    |           |     |           |
    |  Broker   +----+  Broker   +-----+  Broker   |
    |           |    |           |     |           |
    +-----+-----+    +-----+-----+     +-----+-----+
          |                |                 |
          |                |                 |
        /-+-\            /-+-\             /-+-\
       |     |          |     |           |     |
       |     |          |     |           |     |
        \---/            \---/             \---/
     Subscribers      Subscribers       Subscribers

 1. Subscribers are distributed across the brokers, so that each
    broker has a segment of the subscriber population.

 2. Subscriptions (AMQ bindings and subscription queues) are held
    separately for each subscriber on 'their' broker.

 3. Every publisher sends every message to every broker.

It is clear that this model only works where we can exploit a strong
fan-out from publisher to subscriber.  If every published message goes
only to a few subscribers, this model will not provide any advantage.

Failover is relatively easy: all subscribers from the failing broker
reconnect to another broker, resubscribe, and start to collect their
messages.

OpenAMQ Cluster Design
**********************

Cluster Organisation
====================

Defining an OpenAMQ Cluster
---------------------------

An OpenAMQ cluster consists of 1 or more broker processes (also called
"brokers"), running on one or more machines, interconnected by a LAN or
high-reliability WAN links:

- All brokers in a cluster share the same "cluster key", a configured
  string that identifies the cluster.

- All applications that work with the cluster must connect using the
  same cluster key value.  This prevents, for example, an application
  connecting to a production cluster by mistake.

- There is no mechanism for combining two or more separate clusters
  except through external bridging.  Each cluster is a closed set of
  brokers.

The Primary Partition
---------------------

The "primary partition" consists of one or more brokers that are present
at all times except during emergency breakdowns:

- The primary brokers are fully-interconnected using a high-speed and
  high-reliability network.

- The primary brokers should share identical configuration data.

- Brokers are interconnected by exactly one network path: each broker
  creates a single network connection to every other broker using an
  IP address and port specified in the configuration data.

- It is not legal to create multiple independent network paths in the
  cluster, e.g. three independent network paths between three brokers
  A-->B, B-->C, and C-->A, which could lead to different views of the
  network topology if one path breaks.

The Secondary Partition
-----------------------

The "secondary partition" consists of zero or more brokers that may be
activated and de-activated dynamically, without changing the cluster
configuration:

- Each secondary broker is connected to all primary brokers but not to
  other secondary brokers.

- The secondary brokers can usefully share identical configuration data
  with each other, and with the primary brokers.

The Root Broker
---------------

The root broker is elected from the set of primary brokers:

- The root broker is specifically responsible for handling shared queues,
  i.e. queues that can be consumed by multiple applications at the same
  time.

- The root broker is always the most senior primary broker, i.e. the
  broker with the oldest timestamp.

Election Process
----------------

The election of the root broker is a continuous process that happens in
parallel on all primary brokers:

- Each primary broker manages its own election process by monitoring the
  state and seniority of the primary brokers it is connected to.

- A primary broker elects itself as "root" when it receives a unanimous
  vote of all active primary brokers and the electorate constitutes more
  than 50% of the defined primary brokers.

- If any primary broker is still starting (at cluster start time) this
  counts as a "no" vote, and no root will be elected.

- If a primary broker has broken (gone offline), this counts as an
  abstention, and does not affect the election process so long as the
  active primaries form a quorum.

- Secondary brokers do not vote because in a fragmented network this can
  create split-brain scenarios in which more than one broker will elect
  itself as root.

- If the active root broker finds that the electorate constitutes strictly
  less than 50% of the primary brokers, it resigns as root broker.

Failover and Recovery
---------------------

The cluster will, in most cases, manage its state automatically and
safely:

- Election of root is automatic when the cluster starts.

- Failover of root to a backup broker is automatic so long as the cluster
  state is unambiguous.

- When the old root is restarted, it rejoins the primary cluster as a
  backup broker.

- For best results, the primary segment should have three or more brokers
  running in it.

- All primary brokers should be equally capable in terms of resources
  (i.e. running on similarly-specified machines, with similar memory
  and CPU limitations).

The cluster will require manual intervention in some specific cases:

- When the failure of the root leaves the cluster with exactly 50%
  active primary brokers.  I.e., if the primary segment has two
  defined brokers.

- When more than half of the primary brokers crash, and this includes
  the root broker.

The current "intervention" strategy in such cases is manual restarts
of the affected processes.

In the worst case, it may be necessary to remove brokers from the
primary segment (modifying the configuration data) and then restarting
all the cluster processes.

Cluster Management
==================

Application Connection to Cluster
---------------------------------

An application is connected to a single cluster broker:

- Initially, the application will connect to one of the primary brokers.

- The primary broker can redirect the application to a different broker.

- Eventually, applications should be fairly distributed across all
  primary and secondary brokers in the cluster.

- High-volume publishers and criticial consumers may explicitly ask to
  be connected to the current root broker.

Intra-Cluster Connections
-------------------------

The cluster brokers are interconnected using AMQP connections (the
wire-level protocol for AMQ):

- Each broker acts as a client for the brokers it is connected to.

- This relationship is bi-directional, so every broker manages a
  set of inbound client connections from other brokers, and a set
  of outbound proxy connections to other brokers.

- The broker-to-broker connections use a specific login (user type
  = "cluster") that are distinct from normal application logins and
  from console logins.

State Replication
-----------------

The cluster has state, consisting of resources that are created on
brokers at different times, by applications:

- The state is replicated to all brokers in the cluster, each time
  an application makes a change on one broker.

- When a new broker joins the cluster it receives the state from the
  root broker.

- The state is held on each primary broker, so that if the root broker
  dies, the backup which becomes root already has the full state.

- The cluster state has a certain maximum size, defined in the broker
  configuration.

Exchange Management
-------------------

Exchanges are the broker objects to which applications publish messages.
Note that applications may create exchanges dynamically:

- When an application creates an exchange on its broker, that broker
  replicates the "Exchange.Declare" method to all other brokers.

- Exchanges form part of the cluster state.

- There is no guarantee of consistent behaviour if the application
  (for example) creates the same exchange on two brokers at the same
  time, using different properties.

Queue Management
----------------

Queues are the broker objects that hold messages before they are
sent to consumers.  Note that applications create queues dynamically
(the current OpenAMQ implementation does not support durable queues):

- We have distinct management processes for private queues (used to
  store subscription-type messages) and shared queues (used for
  workload distribution).

- A private queue is defined as a queue that is marked as exclusive
  and is automatically deleted when the owning application leaves the
  cluster.

- A private queue is always present on the same broker as the consuming
  application.

- A shared queue is present on every cluster broker but is only active
  on the current root broker.

- When an application creates a private queue on its broker, the queue
  is not replicated.

- When an application creates a shared queue on its broker, the queue
  is replicated to all brokers and forms part of the cluster state.

Queue Bindings
--------------

Queue bindings are the broker objects that link exchanges to queues and
tell the exchanges how to route incoming messages.  Applications create
bindings dynamically (the current OpenAMQ implementation does not
support durable queues):

- We have distinct management processes for bindings on private queues
  and bindings on shared queues.

- When an application creates a binding on a private queue, the broker
  makes a "meta binding" between its instance of the exchange and the
  eponyous exchange on all primary brokers.  The effect of this is to
  route messages from exchange to exchange between brokers.

- When an application creates a binding on a shared queue, the broker
  replicates the binding command to all brokers.  The effect of this
  is to create a local exchange-to-queue binding on all brokers.  Note
  that the shared queue is (assumed to be) present on all brokers.

Queue Consumers
---------------

Queue consumers are the broker objects that link queues to consuming
applications.  Applications always create consumers dynamically, and
there is no concept of "durable consumer" in AMQ:

- We have distinct management processes for consumers on private
  queues and consumers on shared queues.

- A consumer on a private queue works exactly the same way in a
  clustered configuration as in a non-clustered configuration.

- A consumer on a shared queue is proxied to all brokers in the
  cluster.  That is, the original broker becomes a consumer of the
  shared queue on the rest of the cluster.  Note that the shared
  queue is only active on the root broker, but note also that the
  current "root" may move around arbitrarily.

Cluster Message Workflow
========================

Publisher-to-Queue Workflow
---------------------------

An application publishes its messages to a named exchange on its
current broker:

- The exchange will publish the message to queues and to eponynous
  exchanges on other cluster brokers, as defined by its bindings.

- When a message arrives in a shared queue and the current broker is
  not the root broker, the message is pushed to the same exchange on
  the root node instead of being held in the queue.  The root node
  will route the message into the shared queue.

- When a message arrives in a shared queue, on the root, it is held
  and then dispatched to consumers as appropriate.

- Inter-exchange messages (based on meta-bindings) are routed in
  different ways depending on whether the message came from an
  application or from a secondary brokers.

- Messages from applications are routed to all meta-bound exchanges.

- Messages from secondary brokers are routed, by the root broker,
  to all secondary brokers except the originating broker.

The above mechanisms prevent messages from bouncing around the cluster
ad-nauseam:

- A message is never sent back to a broker that originated it.

- Messages are never sent twice to the same broker, even if there are
  multiple bindings for the same message.

Queue-to-Consumer Workflow
--------------------------

A shared queue forwards messages to applications based on the consumers
that it has registered on it:

- Shared queues are always distributed from the root broker, either to
  consumers on that broker, or other brokers.

- Consumers can be local applications, on the current broker.

- Consumers can be remote applications, on a different broker.

- When a non-root broker gets a message from the root server, it passes
  this to its own matching consumer.

